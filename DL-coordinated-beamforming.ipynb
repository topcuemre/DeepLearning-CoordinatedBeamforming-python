{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import DeepMIMO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from scipy.io import loadmat, savemat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Generate Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the default parameters\n",
    "parameters = DeepMIMO.default_params()\n",
    "# Scenario O1_60 extracted at the dataset_folder\n",
    "parameters['dataset_folder'] = r'C:\\\\Users\\\\emre.topcu\\Desktop\\\\ulakws\\datasets\\\\O1_60' # Set DeepMIMO dataset folder that has O1_60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters['num_paths'] = 10\n",
    "# User rows 1-100\n",
    "parameters['user_row_first'] = 1000\n",
    "parameters['user_row_last'] = 1300\n",
    "# Activate only the first basestation\n",
    "parameters['active_BS'] = np.array([3, 4, 5, 6]) \n",
    "\n",
    "parameters['OFDM']['bandwidth'] = 0.5 # 50 MHz\n",
    "parameters['OFDM']['subcarriers'] = 1024 # OFDM with 512 subcarriers\n",
    "parameters['OFDM']['subcarriers_limit'] = 64 # Keep only first 64 subcarriers\n",
    "\n",
    "parameters['ue_antenna']['shape'] = np.array([1, 1, 1]) # Single antenna\n",
    "parameters['bs_antenna']['shape'] = np.array([1, 32, 8]) # ULA of 32 elements\n",
    "parameters['bs_antenna']['radiation_pattern'] = 'halfwave-dipole'\n",
    "parameters['ue_antenna']['radiation_pattern'] = 'halfwave-dipole'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the default parameters\n",
    "for i,j in parameters.items():\n",
    "    print(i,\": ,\", j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "dataset = np.array(DeepMIMO.generate_data(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examination of the dataset\n",
    "print(\"Datatype: \", type(dataset), \"- Items contained: \", len(dataset))\n",
    "print(\"Datatype: \", type(dataset[0]), \"- Items contained: \", len(dataset[0].items()))\n",
    "print(\"Key names: \", list(dataset[0].keys()))\n",
    "print(\"Value names: \", list(dataset[0]['user']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Codebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beamforming_codebook(ant_shape = np.array([1, 32, 1]), oversampling_rate = np.array([1, 1, 1]), kd = 0.5):\n",
    "    \n",
    "    kd = 2 * np.pi * kd\n",
    "    codebook_size = ant_shape * oversampling_rate\n",
    "    \n",
    "    vecs = []\n",
    "    for dim in range(3):\n",
    "        # Transpose\n",
    "        ind = np.arange(ant_shape[dim]).reshape((-1, 1))\n",
    "        codebook_ang = np.linspace(0, np.pi, codebook_size[dim], endpoint = False).reshape((1, -1))                                                                                                     \n",
    "        vec = np.sqrt(1./ant_shape[dim]) * np.exp(-1j * kd * ind * np.cos(codebook_ang))\n",
    "        vecs.append(vec)\n",
    "        \n",
    "    F = np.kron(vecs[2], np.kron(vecs[1], vecs[0]))\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codebook values\n",
    "F = beamforming_codebook(ant_shape = parameters['bs_antenna'][0]['shape'], oversampling_rate = np.array([1, 2, 1]), kd = parameters['bs_antenna'][0]['spacing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_OFDM = int(parameters['OFDM']['subcarriers_limit']/parameters['OFDM']['subcarriers_sampling'])\n",
    "num_beams = F.shape[1]\n",
    "num_bs = len(parameters['active_BS'])\n",
    "num_ue = len(parameters['active_UE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_OFDM)\n",
    "print(num_beams)\n",
    "print(num_bs)\n",
    "print(num_ue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise figure at the base station\n",
    "NF = 5\n",
    "# Channel estimation processing gain          \n",
    "Process_Gain = 10\n",
    "# System bandwidth in Hz\n",
    "BW = parameters['OFDM']['bandwidth'] * 1e9\n",
    "# Noise power in dB\n",
    "noise_power_dB = -204 + 10*np.log10(BW/parameters['OFDM']['subcarriers']) + NF - Process_Gain\n",
    "# Noise power\n",
    "noise_power = 10**(.1*(noise_power_dB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_norm = np.zeros((num_bs, num_ue, num_OFDM), dtype=complex)\n",
    "max_rates = np.zeros((num_bs, num_ue, num_beams))\n",
    "print(\"Shape of input:\", input_norm.shape)\n",
    "print(\"Shape of max-rates:\", max_rates.shape)\n",
    "print(\"Number of UEs: \", len(dataset[0]['user']['channel']))\n",
    "print(\"Shape of channel parameters: \", dataset[0]['user']['channel'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each BS\n",
    "for bs_idx in tqdm(range(num_bs), desc='Neural Network Input-Output Generation-BS', position=0, leave=True):\n",
    "    # Each UE\n",
    "    for ue_idx in tqdm(range(num_ue), desc='Neural Network Input-Output Generation-BS-%i'%bs_idx, position=0, leave=True):\n",
    "        ch = dataset[bs_idx]['user']['channel'][ue_idx].squeeze()\n",
    "        ch = ch + np.sqrt(noise_power) * (np.random.randn(*(ch.shape)) + 1j * np.random.randn(*(ch.shape)))\n",
    "        # Why we set the first two dimensions to 0? (8, 32, 64) --> (1, 1, 64)\n",
    "        input_norm[bs_idx, ue_idx, :] = ch[0, :]\n",
    "        max_rates[bs_idx, ue_idx, :] = np.sum(np.log2(1 + np.abs(ch.T.conj() @ F)**2),  axis = 0)/num_OFDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input reshape - normalize\n",
    "input_norm = np.transpose(input_norm, axes=[1, 0, 2])\n",
    "input_norm = input_norm.reshape((num_ue, -1))\n",
    "input_norm /=  np.amax(np.abs(input_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output reshape - normalize\n",
    "max_rates_norm_factor = np.amax(max_rates, axis=2, keepdims=True)\n",
    "# Do not normalize if all zeros\n",
    "max_rates_norm_factor[max_rates_norm_factor== 0] = 1\n",
    "max_rates /= max_rates_norm_factor\n",
    "max_rates = np.transpose(max_rates, axes=[1, 0, 2])\n",
    "max_rates = max_rates.reshape((num_ue, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./DLCB_dataset'):\n",
    "                      os.makedirs('DLCB_dataset')\n",
    "savemat('./DLCB_dataset/DLCB_input.mat', {'DL_input': input_norm})\n",
    "savemat('./DLCB_dataset/DLCB_output.mat', {'DL_output': max_rates})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from datetime import datetime\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "formatted_date = now.strftime('%Y-%m-%d/%H_%M_%S')\n",
    "print(formatted_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, x_test, y_test,\n",
    "          EPOCHS, BATCH_SIZE, dr, lr, l2_reg,\n",
    "          num_hidden_layers, nodes_per_layer,\n",
    "          loss_fn, n_bs, n_beams, filepath):\n",
    "    \"\"\"\n",
    "    Trains a list of neural network models, one for each base station.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Function to create the model\n",
    "    - x_train: Training input data\n",
    "    - y_train: Training output data\n",
    "    - x_test: Testing input data\n",
    "    - y_test: Testing output data\n",
    "    - EPOCHS: Number of epochs to train the models\n",
    "    - BATCH_SIZE: Batch size for training\n",
    "    - dr: Dropout rate\n",
    "    - lr: Learning rate for the optimizer\n",
    "    - l2_reg: L2 regularization factor (default is 0.01).\n",
    "    - num_hidden_layers: Number of hidden layers in the model\n",
    "    - nodes_per_layer: Number of nodes per hidden layer\n",
    "    - loss_fn: Loss function to use for training\n",
    "    - n_bs: Number of base stations\n",
    "    - n_beams: Number of beams per base station\n",
    "    - filepath: Path to save the best model during training\n",
    "\n",
    "    Returns:\n",
    "    - AP_models: List of trained models, one for each base station\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the input shape by excluding the batch dimension\n",
    "    input_shape = list(x_train.shape[1:])\n",
    "    print(\"Input shape:\", input_shape)\n",
    "    \n",
    "    # Initialize an empty list to store the trained models for each base station\n",
    "    AP_models = []\n",
    "    print(\"------------- y_train shape\", y_train.shape)\n",
    "    \n",
    "    # Iterate over each base station to create and train a model\n",
    "    for bs_idx in range(n_bs):\n",
    "        # Generate a unique identifier for the current base station\n",
    "        idx_str = f'BS{bs_idx}'\n",
    "        idx = bs_idx*n_beams\n",
    "        \n",
    "        # Create a new model for the current base station using the UlakNET\n",
    "        model = UlakNET(input_shape, nodes_per_layer, num_hidden_layers, dr, n_beams, idx_str, l2_reg)\n",
    "        \n",
    "        # Compile the model with the specified learning rate\n",
    "        optimizer = optimizers.Adam(learning_rate=lr)\n",
    "        model.compile(loss=loss_fn, optimizer=optimizer)\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        print(\"y_train shape:\", y_train.shape)\n",
    "        print(\"y_test shape:\", y_test.shape)\n",
    "        print(\"Slicing indices:\")\n",
    "        print(\"y_train slice shape:\", y_train[:, idx:idx + n_beams].shape)\n",
    "        print(\"y_test slice shape:\", y_test[:, idx:idx + n_beams].shape)\n",
    "        \n",
    "        # Train the model with the training data and validate with the test data\n",
    "        model.fit(x_train, y_train[:, idx:idx + n_beams],\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    epochs = EPOCHS,\n",
    "                    verbose = 2,\n",
    "                    validation_data = (x_test, y_test[:,idx:idx + n_beams]),\n",
    "                    callbacks = [\n",
    "                        # Save the best model based on validation loss\n",
    "                        keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto'),\n",
    "                        # Stop training early if validation loss does not improve\n",
    "                        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "                    ])\n",
    "        \n",
    "        # Append the trained model to the list of models\n",
    "        AP_models.append(model)  \n",
    "    return AP_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UlakNET(input_shape, nodes_per_layer, num_hidden_layers, dr, n_beams, idx_str, l2_reg=0.01):\n",
    "    \"\"\"\n",
    "    Creates and returns a Sequential neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    - input_shape: Shape of the input data (excluding batch size).\n",
    "    - nodes_per_layer: Number of nodes in each hidden layer.\n",
    "    - num_hidden_layers: Number of hidden layers.\n",
    "    - dr: Dropout rate.\n",
    "    - n_beams: Number of output units (beams).\n",
    "    - idx_str: Index of a specific BS (Base Station).\n",
    "    - l2_reg: L2 regularization factor (default is 0.01).\n",
    "\n",
    "    Returns:\n",
    "    - model: Compiled Keras Sequential model.\n",
    "    \"\"\"\n",
    "    # Initialize a Sequential model\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Input layer with Dropout\n",
    "    model.add(layers.Dense(nodes_per_layer, activation='relu',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    input_shape=input_shape,\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(l2_reg))\n",
    "                    )\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dr))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Hidden layers\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(layers.Dense(nodes_per_layer, activation='relu',\n",
    "                            kernel_initializer='he_normal',\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(l2_reg))\n",
    "                            )\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.Dropout(dr))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(n_beams,\n",
    "                           activation='linear',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           name=f\"dense_{idx_str}_output\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading input and output sets generated from MATLAB\n",
    "input_set_file=loadmat('DLCB_dataset/DLCB_input.mat')\n",
    "output_set_file=loadmat('DLCB_dataset/DLCB_output.mat')\n",
    "\n",
    "input_set=input_set_file['DL_input']\n",
    "output_set=output_set_file['DL_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "num_user_tot=input_set.shape[0]\n",
    "print(\"Total number of users:\", num_user_tot)\n",
    "n_DL_size=[0.01, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7]\n",
    "cnt=0\n",
    "num_beams=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DL_size_ratio in n_DL_size:\n",
    "    \n",
    "    print(\"For DL size ratio:\", DL_size_ratio)\n",
    "    cnt += 1\n",
    "    DL_size = int(num_user_tot * DL_size_ratio)\n",
    "    print(\"Initializing...\")\n",
    "    \n",
    "    # Train/Test Split\n",
    "    np.random.seed(42)\n",
    "    num_train = int(DL_size * 0.8)\n",
    "    num_test = int(num_user_tot * 0.2)\n",
    "    \n",
    "    # Randomly select train and test indices\n",
    "    train_index = np.random.choice(range(0,num_user_tot), size=num_train, replace=False)\n",
    "    rem_index = set(range(0,num_user_tot))-set(train_index)\n",
    "    test_index= list(set(np.random.choice(list(rem_index), size=num_test, replace=False)))\n",
    "    print(\"DL size: \", DL_size)\n",
    "    print(\"num_train: \", num_train)\n",
    "    print(\"num_test: \", num_test)\n",
    "    print(\"train_size: \", len(train_index))\n",
    "    print(\"rem_size: \", len(rem_index))\n",
    "    print(\"test_size: \", len(test_index))\n",
    "   \n",
    "    # Prepare training and testing data\n",
    "    x_train = np.real(input_set[train_index])\n",
    "    x_test = np.real(input_set[test_index])\n",
    "    y_train = output_set[train_index]\n",
    "    y_test = output_set[test_index]\n",
    "    \n",
    "    print(\"Training data shapes:\")\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"x_test shape: {x_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    print(\"Setting Learning Parameters...\")\n",
    "    # Learning model parameters\n",
    "    epochs = 10\n",
    "    batch_size = 100\n",
    "    dr = 0.05\n",
    "    lr = 0.01\n",
    "    l2_reg = 0.01\n",
    "    num_hidden_layers = 4\n",
    "    nodes_per_layer = x_train.shape[1]\n",
    "    loss_fn = 'mean_squared_error'\n",
    "    filepath = f\"./models/model_{formatted_date}.weights.h5\"\n",
    "    \n",
    "    # Model training\n",
    "    print(\"Starting Model Training...\")\n",
    "    AP_models = train(\n",
    "        UlakNET, x_train, y_train, x_test, y_test,\n",
    "        epochs, batch_size, dr, lr, l2_reg,\n",
    "        num_hidden_layers, nodes_per_layer,\n",
    "        loss_fn, num_bs, num_beams, filepath\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluating Model...\")\n",
    "    DL_Result = {}\n",
    "    \n",
    "    for idx in range(0,num_bs,1): \n",
    "        beams_predicted=AP_models[idx].predict(x_test, batch_size=10, verbose=0)\n",
    "    \n",
    "        DL_Result['TX'+str(idx+1)+'Pred_Beams'] = beams_predicted\n",
    "        DL_Result['TX'+str(idx+1)+'Opt_Beams'] = y_test[:,idx*num_beams:(idx+1)*num_beams]\n",
    "\n",
    "    DL_Result['user_index']=test_index\n",
    "    \n",
    "    \n",
    "    if not os.path.exists('./DLCB_code_output'):\n",
    "                          os.makedirs('DLCB_code_output')\n",
    "    savemat('DLCB_code_output/DL_Result'+str(cnt)+'.mat',DL_Result)\n",
    "    print(\"Iteration completed: \", cnt, \"/\", len(n_DL_size))\n",
    "print(\"Training/Evaluation session finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Read Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = sorted(glob.glob('DLCB_code_output/DL_Result*'), key=lambda x: int(re.findall(r'\\d+', x)[0]))\n",
    "num_files = len(file_list)\n",
    "\n",
    "user_index = []\n",
    "pred_beams = []\n",
    "opt_beams = []\n",
    "for file in tqdm(file_list, desc='Reading DL results'):\n",
    "    matfile = loadmat(file)\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for idx in range(num_bs):\n",
    "        if idx ==2 or idx ==3:\n",
    "            continue\n",
    "        l1.append(matfile['TX'+str(idx+1)+'Pred_Beams'])\n",
    "        l2.append(matfile['TX'+str(idx+1)+'Opt_Beams'])\n",
    "        \n",
    "    pred_beams.append(l1)\n",
    "    opt_beams.append(l2)\n",
    "    user_index.append(matfile['user_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pn = -204 + 10*np.log10(BW) # Noise power in dB\n",
    "SNR = 10**(.1*(0-Pn))\n",
    "\n",
    "ach_rate_DL = np.zeros(num_files)\n",
    "ach_rate_opt = np.zeros(num_files)\n",
    "\n",
    "eff_rate = np.zeros(num_files)\n",
    "opt_rate = np.zeros(num_files)\n",
    "for file_idx in tqdm(np.arange(num_files), desc = 'Calculating results'):\n",
    "    user_index_file = user_index[file_idx].flatten()\n",
    "    for ue_idx in range(len(user_index_file)):\n",
    "        eff_ch = []\n",
    "        opt_ch = []\n",
    "        for bs_idx in range(2):\n",
    "            if file_idx == 0: # Random BF - 0 Samples\n",
    "                pred_beam_idx = np.random.randint(num_beams)\n",
    "            else:\n",
    "                pred_beam_idx = np.argmax(pred_beams[file_idx][bs_idx][ue_idx])\n",
    "            opt_beam_idx = np.argmax(opt_beams[file_idx][bs_idx][ue_idx])\n",
    "            ch_single_bs = dataset[bs_idx]['user']['channel'][user_index_file[ue_idx]].squeeze()\n",
    "            eff_ch_single_pred = ch_single_bs.T.conj() @ F[:, pred_beam_idx]\n",
    "            opt_ch_single_pred = ch_single_bs.T.conj() @ F[:, opt_beam_idx]\n",
    "            eff_ch.append(eff_ch_single_pred)\n",
    "            opt_ch.append(opt_ch_single_pred)\n",
    "        eff_ch = np.array(eff_ch)\n",
    "        opt_ch = np.array(opt_ch)\n",
    "        eff_rate[file_idx] += np.sum(np.log2(1 + SNR * np.abs(np.diag(eff_ch.conj().T @ eff_ch))))\n",
    "        opt_rate[file_idx] += np.sum(np.log2(1 + SNR * np.abs(np.diag(opt_ch.conj().T @ opt_ch))))\n",
    "    eff_rate[file_idx] /= len(user_index_file)*num_OFDM\n",
    "    opt_rate[file_idx] /= len(user_index_file)*num_OFDM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Eff achievable rate calculations\n",
    "theta_user=(102/parameters['bs_antenna'][0]['shape'][1])*np.pi/180\n",
    "alpha=60*np.pi/180\n",
    "distance_user=10\n",
    "Tc_const=(distance_user*theta_user)/(2*np.sin(alpha)) # ms\n",
    "Tt=10*1e-6; # ms\n",
    "\n",
    "v_mph=50\n",
    "v=v_mph*1000*1.6/3600 # m/s\n",
    "Tc=Tc_const/v\n",
    "\n",
    "overhead_opt=1-(num_beams*Tt)/Tc # overhead of beam training\n",
    "overhead_DL=1-Tt/Tc # overhead of proposed DL method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_size_array=np.arange(0, 2.5*(num_files), 2.5);\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(DL_size_array, opt_rate, '--k', label = 'Genie-aided Coordinated Beamforming')\n",
    "plt.plot(DL_size_array, eff_rate*overhead_DL, '-bo', label = 'Deep Learning Coordinated Beamforming')\n",
    "plt.plot(DL_size_array, opt_rate*overhead_opt, '-rs', label = 'Baseline Coordinated Beamforming')\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 35])\n",
    "plt.minorticks_on()\n",
    "plt.grid()\n",
    "plt.xlabel('Deep Learning Dataset Size (Thousand Samples)')\n",
    "plt.ylabel('Achievable Rate (bps/Hz)')\n",
    "plt.legend()\n",
    "plt.savefig('result.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
